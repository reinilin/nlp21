{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW_3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dbamman/nlp21/blob/main/HW3/HW_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlFS_v7TpRHe"
      },
      "source": [
        "# Homework 3: Pytorch and CNNs\n",
        "\n",
        "In this homework, you will begin exploring Pytorch, a neural network library that will be used throughout the remainder of the semester.  This homework will focus on Convolutional Neural Networks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyY0yl1Jpf2o"
      },
      "source": [
        "import sys, argparse\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "import csv\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import nltk\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "#Sets random seeds for reproducibility\n",
        "seed=159259\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7DVjxeq_-OF"
      },
      "source": [
        "!python -m nltk.downloader punkt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlYfHZwlQXA_"
      },
      "source": [
        "When looking up pytorch documentation, it may be useful to know which version of torch you are running.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUdEHON5lybF"
      },
      "source": [
        "print(torch.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xrbyc1flKp_"
      },
      "source": [
        "# **IMPORTANT**: GPU is not enabled by default\n",
        "\n",
        "You must switch runtime environments if your output of the next block of code has an error saying \"ValueError: Expected a cuda device, but got: cpu\"\n",
        "\n",
        "Go to Runtime > Change runtime type > Hardware accelerator > GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRy4VWrvkCP6"
      },
      "source": [
        "device = torch.cuda.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Running on {}\".format(device))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyARzkPKmUlR"
      },
      "source": [
        "# Data Processing\n",
        "\n",
        "Let's begin by loading our datasets and the 50-dimensional GLoVE word embeddings.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_ZZQsGwH5vj"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/dbamman/nlp21/main/HW3/acl.train\n",
        "!wget https://raw.githubusercontent.com/dbamman/nlp21/main/HW3/acl.dev\n",
        "!wget https://raw.githubusercontent.com/dbamman/nlp21/main/HW3/glove.6B.50d.50K.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vC5tWWn2mWhH"
      },
      "source": [
        "trainingFile = \"acl.train\"\n",
        "devFile = \"acl.dev\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_vLcPzzIxDw"
      },
      "source": [
        "labels = {'APPLICATIONS': 11,\n",
        " 'CSSCA': 23,\n",
        " 'DIALOGUE': 12,\n",
        " 'DISCOURSE': 13,\n",
        " 'ETHICS': 8,\n",
        " 'GENERATION': 9,\n",
        " 'GREEN': 15,\n",
        " 'GROUNDING': 18,\n",
        " 'IE': 6,\n",
        " 'INTERPRET': 10,\n",
        " 'IR': 22,\n",
        " 'LEXSEM': 7,\n",
        " 'LING': 24,\n",
        " 'MLCLASS': 1,\n",
        " 'MLLM': 16,\n",
        " 'MT': 4,\n",
        " 'MULTILING': 3,\n",
        " 'OTHER': 25,\n",
        " 'PHON': 5,\n",
        " 'QA': 17,\n",
        " 'RESOURCES': 14,\n",
        " 'SA': 21,\n",
        " 'SENTSEM': 0,\n",
        " 'SPEECH': 19,\n",
        " 'SUMM': 2,\n",
        " 'SYNTAX': 20}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNb4H1auI4lA"
      },
      "source": [
        "def get_batches(x, y, xType, batch_size=12):\n",
        "    batches_x=[]\n",
        "    batches_y=[]\n",
        "    for i in range(0, len(x), batch_size):\n",
        "        batches_x.append(xType(x[i:i+batch_size]))\n",
        "        batches_y.append(torch.LongTensor(y[i:i+batch_size]))\n",
        "    \n",
        "    return batches_x, batches_y\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnIbufFHlYSx"
      },
      "source": [
        "PAD_INDEX = 0             # reserved for padding words\n",
        "UNKNOWN_INDEX = 1         # reserved for unknown words\n",
        "SEP_INDEX = 2\n",
        "\n",
        "data_lens = []\n",
        "\n",
        "def read_embeddings(filename, vocab_size=50000):\n",
        "  \"\"\"\n",
        "  Utility function, loads in the `vocab_size` most common embeddings from `filename`\n",
        "  \n",
        "  Arguments:\n",
        "  - filename:     path to file\n",
        "                  automatically infers correct embedding dimension from filename\n",
        "  - vocab_size:   maximum number of embeddings to load\n",
        "\n",
        "  Returns \n",
        "  - embeddings:   torch.FloatTensor matrix of size (vocab_size x word_embedding_dim)\n",
        "  - vocab:        dictionary mapping word (str) to index (int) in embedding matrix\n",
        "  \"\"\"\n",
        "\n",
        "  # get the embedding size from the first embedding\n",
        "  with open(filename, encoding=\"utf-8\") as file:\n",
        "    word_embedding_dim = len(file.readline().split(\" \")) - 1\n",
        "\n",
        "  vocab = {}\n",
        "\n",
        "  embeddings = np.zeros((vocab_size, word_embedding_dim))\n",
        "  with open(filename, encoding=\"utf-8\") as file:\n",
        "    for idx, line in enumerate(file):\n",
        "\n",
        "      if idx + 2 >= vocab_size:\n",
        "        break\n",
        "\n",
        "      cols = line.rstrip().split(\" \")\n",
        "      val = np.array(cols[1:])\n",
        "      word = cols[0]\n",
        "      embeddings[idx + 2] = val\n",
        "      vocab[word] = idx + 2\n",
        "  \n",
        "  # a FloatTensor is a multidimensional matrix\n",
        "  # that contains 32-bit floats in every entry\n",
        "  # https://pytorch.org/docs/stable/tensors.html\n",
        "  return torch.FloatTensor(embeddings), vocab\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrBHMiLPIOKB"
      },
      "source": [
        "# Logistic regression\n",
        "\n",
        "First, let's code up logistic regression in pytorch so you can see how the general framework works, and also get a sense of baseline performance that we can compare a CNN against."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7A8N5GNJH4x8"
      },
      "source": [
        "def get_vocab(filename, max_words=10000):\n",
        "    unigram_counts=Counter()\n",
        "    with open(filename) as file:    \n",
        "        for line in file:\n",
        "            cols=line.rstrip().split(\"\\t\")\n",
        "            idd = cols[0]\n",
        "            label = cols[1]\n",
        "            title = cols[2]\n",
        "            abstract = cols[3]\n",
        "            strr=\"%s %s\" % (title, abstract)\n",
        "            words=nltk.word_tokenize(strr)\n",
        "\n",
        "            for word in words:\n",
        "                word=word.lower()\n",
        "                unigram_counts[word]+=1\n",
        "\n",
        "    vocab={}\n",
        "    for k,v in unigram_counts.most_common(max_words):\n",
        "        vocab[k]=len(vocab)\n",
        "    return vocab\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "392D8YLfI_K3"
      },
      "source": [
        "class LogisticRegressionClassifier(nn.Module):\n",
        "\n",
        "   def __init__(self, input_dim, output_dim):\n",
        "      super().__init__()\n",
        "      self.linear = torch.nn.Linear(input_dim, output_dim)\n",
        " \n",
        "    \n",
        "   def forward(self, input): \n",
        "      x1 = self.linear(input)\n",
        "      return x1\n",
        "\n",
        "   def evaluate(self, x, y):\n",
        "\n",
        "      self.eval()\n",
        "      corr = 0.\n",
        "      total = 0.\n",
        "      with torch.no_grad():\n",
        "        for x, y in zip(x, y):\n",
        "          y_preds=self.forward(x)\n",
        "          for idx, y_pred in enumerate(y_preds):\n",
        "              prediction=torch.argmax(y_pred)\n",
        "              if prediction == y[idx]:\n",
        "                corr += 1.\n",
        "              total+=1                          \n",
        "      return corr/total\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgaDKtOrc10l"
      },
      "source": [
        "## Average Embedding Representation\n",
        "Let's train a logistic regression classifier where the input is the average GLoVE embedding for all words in a paper's title and abstract"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YgU4027luO3"
      },
      "source": [
        "def read_glove_data(filename, vocab, embs):\n",
        "    data=[]\n",
        "    data_labels=[]\n",
        "    with open(filename) as file:\n",
        "        for line in file:\n",
        "            avg_emb=np.zeros(50)\n",
        "            cols=line.rstrip().split(\"\\t\")\n",
        "            idd = cols[0]\n",
        "            label = cols[1]\n",
        "            title = cols[2]\n",
        "            abstract = cols[3]\n",
        "            strr=\"%s %s\" % (title, abstract)\n",
        "            words=nltk.word_tokenize(strr)\n",
        "            avg_counter = 0.\n",
        "            for word in words:\n",
        "                word=word.lower()\n",
        "                if word in glove_vocab:\n",
        "                    avg_emb += embs[glove_vocab[word]].numpy()\n",
        "                    avg_counter += 1.\n",
        "            avg_emb /= avg_counter\n",
        "\n",
        "            data.append(avg_emb)\n",
        "            data_labels.append(labels[label])\n",
        "    return data, data_labels "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYb1iVsqb0Le"
      },
      "source": [
        "embs, glove_vocab = read_embeddings(\"glove.6B.50d.50K.txt\")\n",
        "avg_train_x, avg_train_y=read_glove_data(trainingFile, glove_vocab, embs)\n",
        "avg_dev_x, avg_dev_y=read_glove_data(devFile, glove_vocab, embs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FeYYf7-c01Z"
      },
      "source": [
        "avg_trainX, avg_trainY=get_batches(avg_train_x, avg_train_y, xType=torch.FloatTensor)\n",
        "avg_devX, avg_devY=get_batches(avg_dev_x, avg_dev_y, xType=torch.FloatTensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Duzn0vCrdR5X"
      },
      "source": [
        "logreg=LogisticRegressionClassifier(50, len(labels))\n",
        "optimizer = torch.optim.Adam(logreg.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "losses = []\n",
        "cross_entropy=nn.CrossEntropyLoss()\n",
        "\n",
        "num_labels=len(labels)\n",
        "\n",
        "for epoch in range(200):\n",
        "    logreg.train()\n",
        "    \n",
        "    for x, y in zip(avg_trainX, avg_trainY):\n",
        "        y_pred=logreg.forward(x)\n",
        "        loss = cross_entropy(y_pred.view(-1, num_labels), y.view(-1))\n",
        "        losses.append(loss)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    dev_accuracy=logreg.evaluate(avg_devX, avg_devY)\n",
        "    if epoch % 5 == 0:\n",
        "        print(\"Epoch %s, dev accuracy: %.3f\" % (epoch, dev_accuracy))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObjO1BiXc_nY"
      },
      "source": [
        "## BOW Representation\n",
        "Feel free to fill in your bag-of-words implementation into read_bow_data() to see how the logistic classifier model works with a different featurization.  (You are not required to do anything within this BOW representation section; we provide the structure in case you'd like to explore how your your BOW logistic regression model from the last homework could be implemented in Pytorch)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huWSY2FNlqrF"
      },
      "source": [
        "def read_bow_data(filename, vocab):\n",
        "    data=[]\n",
        "    data_labels=[]\n",
        "    with open(filename) as file:\n",
        "        for line in file:\n",
        "            cols=line.rstrip().split(\"\\t\")\n",
        "            idd = cols[0]\n",
        "            label = cols[1]\n",
        "            title = cols[2]\n",
        "            abstract = cols[3]\n",
        "            strr=\"%s %s\" % (title, abstract)\n",
        "            bow=np.zeros(len(vocab))\n",
        "\n",
        "            '''\n",
        "            Insert your bow code here to store the featurization in the bow variable \n",
        "            \n",
        "            '''\n",
        "\n",
        "            data.append(bow)\n",
        "\n",
        "            data_labels.append(labels[label])\n",
        "    return data, data_labels \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tc9ZGJvWImkA"
      },
      "source": [
        "bow_vocab=get_vocab(trainingFile)\n",
        "bow_train_x, bow_train_y=read_bow_data(trainingFile, bow_vocab)\n",
        "bow_dev_x, bow_dev_y=read_bow_data(devFile, bow_vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFucMsZII8Hb"
      },
      "source": [
        "bow_trainX, bow_trainY=get_batches(bow_train_x, bow_train_y, xType=torch.FloatTensor)\n",
        "bow_devX, bow_devY=get_batches(bow_dev_x, bow_dev_y, xType=torch.FloatTensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byr4SJB1JCDO"
      },
      "source": [
        "logreg=LogisticRegressionClassifier(len(bow_vocab), len(labels))\n",
        "optimizer = torch.optim.Adam(logreg.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "losses = []\n",
        "cross_entropy=nn.CrossEntropyLoss()\n",
        "\n",
        "num_labels=len(labels)\n",
        "\n",
        "for epoch in range(200):\n",
        "    for x, y in zip(bow_trainX, bow_trainY):\n",
        "        y_pred=logreg.forward(x)\n",
        "        loss = cross_entropy(y_pred.view(-1, num_labels), y.view(-1))\n",
        "        losses.append(loss)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    dev_accuracy=logreg.evaluate(bow_devX, bow_devY)\n",
        "    if epoch % 5 == 0:\n",
        "        print(\"Epoch %s, dev accuracy: %.3f\" % (epoch, dev_accuracy))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbHrmE4jJQrT"
      },
      "source": [
        "# Deliverable 1. CNN \n",
        "\n",
        "Now let's create our CNN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YhST7BOJPoG"
      },
      "source": [
        "def read_data(filename, vocab, labels):\n",
        "    \"\"\"\n",
        "    :param filename: the name of the file\n",
        "    :return: list of tuple ([word index list], label)\n",
        "    as input for the forward and backward function\n",
        "    \"\"\"    \n",
        "    data = []\n",
        "    data_labels = []\n",
        "    file = open(filename)\n",
        "    for line in file:\n",
        "        cols = line.split(\"\\t\")\n",
        "        idd = cols[0]\n",
        "        label = cols[1]\n",
        "        title = cols[2]\n",
        "        abstract = cols[3]\n",
        "        w_int = []\n",
        "        for w in nltk.word_tokenize(title.lower()):\n",
        "            if w in vocab:\n",
        "                w_int.append(vocab[w])\n",
        "            else:\n",
        "                w_int.append(UNKNOWN_INDEX)\n",
        "        w_int.append(SEP_INDEX)\n",
        "        w_int.append(SEP_INDEX)\n",
        "        for w in nltk.word_tokenize(abstract.lower()):\n",
        "            if w in vocab:\n",
        "                w_int.append(vocab[w])\n",
        "            else:\n",
        "                w_int.append(UNKNOWN_INDEX)\n",
        "        data_lens.append(len(w_int))\n",
        "        if len(w_int) < 549:\n",
        "            w_int.extend([PAD_INDEX] * (549 - len(w_int)))\n",
        "        if len(w_int) < 550:\n",
        "          data.append((w_int))\n",
        "          data_labels.append(labels[label])\n",
        "    file.close()\n",
        "    return data, data_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sB60ratJZvB"
      },
      "source": [
        "embs, cnn_vocab = read_embeddings(\"glove.6B.50d.50K.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hStl2tmiJesV"
      },
      "source": [
        "cnn_train_x, cnn_train_y = read_data(trainingFile, cnn_vocab, labels)\n",
        "cnn_dev_x, cnn_dev_y = read_data(devFile, cnn_vocab, labels)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZNvT-REJgv1"
      },
      "source": [
        "cnn_trainX, cnn_trainY=get_batches(cnn_train_x, cnn_train_y, torch.LongTensor)\n",
        "cnn_devX, cnn_devY=get_batches(cnn_dev_x, cnn_dev_y, torch.LongTensor)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDGz8mqdJjic"
      },
      "source": [
        "class CNNClassifier(nn.Module):\n",
        "\n",
        "   def __init__(self, params, pretrained_embeddings):\n",
        "      super().__init__()\n",
        "      self.seq_len = params[\"max_seq_len\"]\n",
        "      self.num_labels = params[\"label_length\"]\n",
        "      \n",
        "      '''\n",
        "      Initialize the following layers according to the hw spec\n",
        "      '''\n",
        "      self.embeddings = ...\n",
        "\n",
        "      # convolution over 1 word\n",
        "      self.conv_1 = nn.Conv1d(...)\n",
        "      self.pool_1 = nn.MaxPool1d(...)\n",
        "\n",
        "      # convolution over 2 words    \n",
        "      self.conv_2 = nn.Conv1d(...)\n",
        "      self.pool_2 = nn.MaxPool1d(...)\n",
        "        \n",
        "      # convolution over 3 words\n",
        "      self.conv_3 = nn.Conv1d(...)\n",
        "      self.pool_3 = nn.MaxPool1d(...)\n",
        "        \n",
        "      self.fc = ...\n",
        "\n",
        "\n",
        "    \n",
        "   def forward(self, input): \n",
        "      #embeds the input sequences\n",
        "      x0 = self.embeddings(input)\n",
        "      #changes dimensions to be consistent with conv1d\n",
        "      x0 = x0.permute(0, 2, 1)\n",
        "\n",
        "      '''\n",
        "      Create the hidden representations according to the hw spec\n",
        "      '''\n",
        "      #Apply the one-word convolution, tanh, and pool\n",
        "      x1 = ...\n",
        "    \n",
        "      #Apply the two-word convolution, tanh, and pool\n",
        "      x2 = ...\n",
        "        \n",
        "      #Apply the three-word convolution, tanh, and pool\n",
        "      x3 = ...\n",
        "\n",
        "      #Concatenates the output of all 3 convolution layers\n",
        "      combined=...\n",
        "\n",
        "      #Connects the combined output to the fully-connected layer\n",
        "      out = ...\n",
        "      return out.squeeze()\n",
        "\n",
        "   def evaluate(self, x, y):\n",
        "      \n",
        "      self.eval()\n",
        "      corr = 0.\n",
        "      total = 0.\n",
        "\n",
        "      with torch.no_grad():\n",
        "\n",
        "        for x, y in zip(x, y):\n",
        "          y_preds=self.forward(x)\n",
        "          for idx, y_pred in enumerate(y_preds):\n",
        "              prediction=torch.argmax(y_pred)\n",
        "              if prediction == y[idx]:\n",
        "                corr += 1.\n",
        "              total+=1                          \n",
        "      return corr/total\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxrBo0N0JlGM"
      },
      "source": [
        "embs, cnn_vocab = read_embeddings(\"glove.6B.50d.50K.txt\")\n",
        "cnnmodel = CNNClassifier(params={\"max_seq_len\": 549, \"label_length\": len(labels)}, pretrained_embeddings=embs)\n",
        "optimizer = torch.optim.Adam(cnnmodel.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "losses = []\n",
        "cross_entropy=nn.CrossEntropyLoss()\n",
        "\n",
        "num_epochs=25\n",
        "best_dev_acc = 0.\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    cnnmodel.train()\n",
        "\n",
        "    for x, y in zip(cnn_trainX, cnn_trainY):\n",
        "      y_pred = cnnmodel.forward(x)\n",
        "      loss = cross_entropy(y_pred.view(-1, cnnmodel.num_labels), y.view(-1))\n",
        "      losses.append(loss) \n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    dev_accuracy=cnnmodel.evaluate(cnn_devX, cnn_devY)\n",
        "    if epoch % 1 == 0:\n",
        "        print(\"Epoch %s, dev accuracy: %.3f\" % (epoch, dev_accuracy))\n",
        "        if dev_accuracy > best_dev_acc:\n",
        "          torch.save(cnnmodel.state_dict(), 'best-cnnmodel-parameters.pt')\n",
        "          best_dev_acc = dev_accuracy\n",
        "\n",
        "cnnmodel.load_state_dict(torch.load('best-cnnmodel-parameters.pt'))\n",
        "print(\"\\nBest Performing Model achieves dev accuracy of : %.3f\" % (best_dev_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8j5kM7T5T69d"
      },
      "source": [
        "# Model Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7u2MerXT9fb"
      },
      "source": [
        "## Loss Examination\n",
        "To debug your model and ensure it is updating correctly, it may be helpful to visualize your training loss.  The following code plots loss over epoch.  This should decrease as the model trains and eventually converge.  If your training loss is not decreasing, you might not be initializing your model or creating your forward() pass correctly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0W1WDYkfrdLX"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(range(len(losses)), losses)\n",
        "plt.title(\"Training Loss over Time\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjSCeYlpcw9v"
      },
      "source": [
        "# Deliverable 2: Explore NLP articles\n",
        "Now that you have your CNN trained, let's go ahead and make predictions for all of the 7,188 abstracts in our full dataset of NLP papers published between 2013-2020. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X55A-oFlaSaE"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/dbamman/nlp21/main/HW3/acl.all.tsv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yC-1-sXdOn2g"
      },
      "source": [
        "def read_prediction_data(filename, vocab):\n",
        "    \"\"\"\n",
        "    :param filename: the name of the file\n",
        "    :return: list of tuple ([word index list], label)\n",
        "    as input for the forward and backward function\n",
        "    \"\"\"    \n",
        "    data = []\n",
        "    data_dates = []\n",
        "    file = open(filename)\n",
        "    for line in file:\n",
        "        cols = line.split(\"\\t\")\n",
        "        idd = cols[0]\n",
        "        year = int(cols[1])\n",
        "        title = cols[2]\n",
        "        abstract = cols[3]\n",
        "        w_int = []\n",
        "        for w in nltk.word_tokenize(title.lower()):\n",
        "            # skip the unknown words\n",
        "            if w in vocab:\n",
        "                w_int.append(vocab[w])\n",
        "            else:\n",
        "                w_int.append(UNKNOWN_INDEX)\n",
        "        w_int.append(SEP_INDEX)\n",
        "        w_int.append(SEP_INDEX)\n",
        "        for w in nltk.word_tokenize(abstract.lower()):\n",
        "            # skip the unknown words\n",
        "            if w in vocab:\n",
        "                w_int.append(vocab[w])\n",
        "            else:\n",
        "                w_int.append(UNKNOWN_INDEX)\n",
        "        data_lens.append(len(w_int))\n",
        "        if len(w_int) < 549:\n",
        "            w_int.extend([PAD_INDEX] * (549 - len(w_int)))\n",
        "        if len(w_int) < 550:\n",
        "          data.append((w_int))\n",
        "          data_dates.append(year)\n",
        "    file.close()\n",
        "    return data, data_dates"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btuKqE8LN6yS"
      },
      "source": [
        "predictFile=\"acl.all.tsv\"\n",
        "cnn_test_x, cnn_test_dates = read_prediction_data(predictFile, cnn_vocab)\n",
        "cnn_predictX, cnn_predictDates=get_batches(cnn_test_x, cnn_test_dates, torch.LongTensor, batch_size=256)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRa093KsPUYH"
      },
      "source": [
        "reverse_labels={labels[k]:k for k in labels}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzNnW1HHdhFO"
      },
      "source": [
        "Now let's make predictions on all of that data with your trained `cnnmodel`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqpEWS69Oi8i"
      },
      "source": [
        "with torch.no_grad():\n",
        "\n",
        "  all_dates=[]\n",
        "  all_preds=[]\n",
        "  for x, y in zip(cnn_predictX, cnn_predictDates):\n",
        "    y_preds=cnnmodel.forward(x)\n",
        "    for idx, y_pred in enumerate(y_preds):\n",
        "        prediction=int(torch.argmax(y_pred))\n",
        "        all_dates.append(int(y[idx]))\n",
        "        all_preds.append(prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvabtt6XdLnl"
      },
      "source": [
        "What are the most frequent categories among our predictions?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qvbk-FBnO_aD"
      },
      "source": [
        "from collections import Counter\n",
        "cat_counts=Counter()\n",
        "for val in all_preds:\n",
        "  cat_counts[val]+=1\n",
        "\n",
        "for k,v in cat_counts.most_common():\n",
        "  print(v, reverse_labels[k])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryVHtoQXdPFs"
      },
      "source": [
        "Now let's plot the frequency with which any given category appears over time by aggregating those predictions by the year in which the corresponding papers were published."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRGk370CPxXx"
      },
      "source": [
        "minYear=min(all_dates)\n",
        "maxYear=max(all_dates)\n",
        "counts=np.zeros((maxYear-minYear+1, len(labels)))\n",
        "for year, pred in zip(all_dates, all_preds):\n",
        "  counts[year-minYear][pred]+=1\n",
        "counts=counts/np.sum(counts,axis=1)[:, np.newaxis]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V14d_5zxQZRC"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_category(cats, labels):\n",
        "  for cat in cats:\n",
        "    data=[]\n",
        "    for idx, val in enumerate(counts[:,labels[cat]]):\n",
        "      data.append(val)\n",
        "    plt.plot(range(2013,2021), data)\n",
        "  plt.legend(cats)\n",
        "  plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GaSa5V0Qx-F"
      },
      "source": [
        "plot_category([\"MT\", \"SA\", \"GENERATION\", \"ETHICS\"], labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIpNqwbWWRaS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAW6FujgeUOb"
      },
      "source": [
        "Should we trust these results as reflecting trends in the attention the ACL community gives to these topics?  Think about the potential biases that might exist in this method and the results, especially given your experience in creating this dataset.  Explore this model and data with whatever methods you think would help your argument -- e.g., try plotting a confusion matrix over the development data to see which classes are being confused, examine the data points that have the highest confidence wrong predictions, etc.).  How would you go about interrogating this method to know whether to trust these findings?  Submit your <200 word answer to this question as a PDF on gradescope.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDYPkPdBfAQd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}